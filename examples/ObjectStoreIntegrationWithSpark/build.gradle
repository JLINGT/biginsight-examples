import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.FileSystem
import org.apache.hadoop.fs.Path
import org.apache.hadoop.fs.RemoteIterator
import com.ibm.stocator.fs.swift.SwiftAPIClient

buildscript {
    repositories {
        mavenCentral()
        jcenter()
    }
    dependencies {
        classpath 'com.ibm.stocator:stocator:1.0.2'
        classpath 'org.apache.hadoop:hadoop-mapreduce-client-core:2.2.0'
        classpath 'org.apache.hadoop:hadoop-common:2.2.0'
        // uberjar
        classpath 'com.github.jengelman.gradle.plugins:shadow:1.2.2'
    }
}

plugins {
  id 'org.hidetake.ssh' version '1.5.0'
  id 'java'
}

// set the dependencies for compiling the groovy script and mapreduce classes
repositories {
    mavenCentral()
    jcenter()
}

dependencies {
    // the spark swift library
    compile 'com.ibm.stocator:stocator:1.0.2'
}


Properties props = new Properties()
props.load(new FileInputStream("$projectDir/../../connection.properties"))

// extract BigInsights hostname from the gateway url
def matcher = props.gateway =~ /^(https?:\/\/)([^:^\/]*)(:\d*)?(.*)?.*$/
def hostname = matcher[0][2] 


// setup the connection details for ssh
remotes {
    bicluster {
       host = hostname
       user = props.username
       password = props.password
    }
}

ssh.settings {
    if (props.known_hosts == 'allowAnyHosts') {
        // disable ssh host key verification 
        knownHosts = allowAnyHosts
    }
}

task('customClean') << {
    delete './lib'
}

// ensure gradle's standard clean target executes out customClean target
clean.dependsOn customClean

// create jar file with all dependencies
task('SetupLibs', type: Jar) {
    baseName = 'stocator-library-with-dependencies'
    from { configurations.compile.collect { it.isDirectory() ? it : zipTree(it) } }
    with jar
}

def deleteContainer(os_auth_url, os_tenant, os_username, os_password, os_region, os_auth_method, os_container_name) {

    def mConf = new Configuration(true)

    mConf.set("fs.swift2d.impl","com.ibm.stocator.fs.ObjectStoreFileSystem")

    def prefix = "fs.swift2d.service." + os_region

    mConf.set(prefix + ".auth.url",     os_auth_url)
    mConf.set(prefix + ".public",       "true")
    mConf.set(prefix + ".tenant",       os_tenant)
    mConf.set(prefix + ".username",     os_username)
    mConf.set(prefix + ".password",     os_password)
    mConf.set(prefix + ".auth.method",  os_auth_method)
    mConf.set(prefix + ".region",       os_region)

    def publicURL = "swift2d://${os_container_name}.${os_region}"
    def fs = org.apache.hadoop.fs.FileSystem.get(URI.create(publicURL), mConf)

    def path = new Path("${publicURL}/counts")
    println "Attempting to delete ${path}"

    // delete objects in container
    if (!fs.delete(path)) {
        throw new GradleException("Unable to delete ${path}")
    }

    // delete container
    def client = new SwiftAPIClient(URI.create(publicURL), mConf)
    client.account.getContainer(os_container_name).delete()

}


task('ExamplePush', dependsOn: SetupLibs) {

    doLast {
        
        // temporary identifier for tables, folders, etc 
        def tmpId = "${new Date().getTime()}"

        // temporary folder name on cluster
        def tmpDir = "test-${tmpId}"

        // temporary hdfs dir
        def tmpHdfsDir = "/user/${props.username}/${tmpDir}"

        // ssh plugin documentation: https://gradle-ssh-plugin.github.io/docs/
        ssh.run {
            session(remotes.bicluster) {

                try {
                    // initialise kerberos
                    execute "kinit -k -t ${props.username}.keytab ${props.username}@IBM.COM"
                } 
                catch (Exception e) {
                    println "problem running kinit - maybe this is a Basic cluster?"
                }

                // create temp local dir for holding sparkscript and jars
                execute "mkdir ${tmpDir}"

                // upload spark script
                put from: "${projectDir}/exporttoswift.py", into: "${tmpDir}/exporttoswift.py"
                put from: "${projectDir}/LICENSE", into: "${tmpDir}/LICENSE"
                put from: "${projectDir}/build/libs/stocator-library-with-dependencies.jar", into: "${tmpDir}/stocator-library-with-dependencies.jar"

                // create temp hdfs folder for holding LICENSE file
                execute "hadoop fs -mkdir ${tmpHdfsDir}"

                // put LICENSE into hdfs
                execute "hadoop fs -put ${tmpDir}/LICENSE ${tmpHdfsDir}/LICENSE"

                def jars = "--jars ${tmpDir}/stocator-library-with-dependencies.jar"

                def os_auth_url    = props.objectstore_auth_url
                def os_tenant      = props.objectstore_tenant
                def os_username    = props.objectstore_username
                def os_password    = props.objectstore_password
                def os_region      = props.objectstore_region
                def os_auth_method = props.objectstore_auth_method
                def os_container   = tmpId
 
                // execute spark job
                execute "pyspark ${jars} ${tmpDir}/exporttoswift.py ${tmpHdfsDir}/LICENSE '${os_auth_url}' '${os_tenant}' '${os_username}' '${os_password}' '${os_region}' '${os_auth_method}' '${os_container}'"

                // remove temporary hdfs dir
                execute "hadoop fs -rm -r ${tmpHdfsDir}"

                // remove temporary local dir
                execute "rm -rf ${tmpDir}"

                // delete container       
                project.deleteContainer(os_auth_url, os_tenant, os_username, os_password, os_region, os_auth_method, os_container)

                println "\nSUCCESS >> test successful processing and uploading data to object store"
            }
        }
    }
}

task('ExamplePull', dependsOn: SetupLibs) {

    doLast {
        
        // temporary identifier for tables, folders, etc 
        def tmpId = "${new Date().getTime()}"

        // temporary folder name on cluster
        def tmpDir = "test-${tmpId}"

        // temporary hdfs dir
        def tmpHdfsDir = "/user/${props.username}/${tmpDir}"

        // ssh plugin documentation: https://gradle-ssh-plugin.github.io/docs/
        ssh.run {
            session(remotes.bicluster) {

                try {
                    // initialise kerberos
                    execute "kinit -k -t ${props.username}.keytab ${props.username}@IBM.COM"
                } 
                catch (Exception e) {
                    println "problem running kinit - maybe this is a Basic cluster?"
                }

                // create temp local dir for holding sparkscript and jars
                execute "mkdir ${tmpDir}"

                // upload spark script
                put from: "${projectDir}/exporttoswift.py", into: "${tmpDir}/exporttoswift.py"
                put from: "${projectDir}/importfromswift.py", into: "${tmpDir}/importfromswift.py"
                put from: "${projectDir}/LICENSE", into: "${tmpDir}/LICENSE"
                put from: "${projectDir}/build/libs/stocator-library-with-dependencies.jar", into: "${tmpDir}/stocator-library-with-dependencies.jar"

                // create temp hdfs folder for holding LICENSE file
                execute "hadoop fs -mkdir ${tmpHdfsDir}"

                // put LICENSE into hdfs
                execute "hadoop fs -put ${tmpDir}/LICENSE ${tmpHdfsDir}/LICENSE"

                def jars = "--jars ${tmpDir}/stocator-library-with-dependencies.jar"

                def os_auth_url    = props.objectstore_auth_url
                def os_tenant      = props.objectstore_tenant
                def os_username    = props.objectstore_username
                def os_password    = props.objectstore_password
                def os_region      = props.objectstore_region
                def os_auth_method = props.objectstore_auth_method
                def os_container   = tmpId

                // execute spark job to export from hdfs to swift
                execute "pyspark ${jars} ${tmpDir}/exporttoswift.py ${tmpHdfsDir}/LICENSE '${os_auth_url}' '${os_tenant}' '${os_username}' '${os_password}' '${os_region}' '${os_auth_method}' '${os_container}'"
                
                // clean up temporary hdfs dir - we will import the files from swift to here
                execute "hadoop fs -rm -r ${tmpHdfsDir}"

                // execute spark job to import from swift to hdfs
                execute "pyspark ${jars} ${tmpDir}/importfromswift.py ${tmpHdfsDir} '${os_auth_url}' '${os_tenant}' '${os_username}' '${os_password}' '${os_region}' '${os_auth_method}' '${os_container}'"

                // print contents of files imported from swift
                execute "hadoop fs -ls ${tmpHdfsDir}/"
                execute "hadoop fs -cat ${tmpHdfsDir}/*"

                // remove temporary hdfs dir
                execute "hadoop fs -rm -r ${tmpHdfsDir}"

                // remove temporary local dir
                execute "rm -rf ${tmpDir}"
       
                // delete container       
                project.deleteContainer(os_auth_url, os_tenant, os_username, os_password, os_region, os_auth_method, os_container)

                println "\nSUCCESS >> test successful importing data from object store"
            }
        }
    }
}
task('Example') {
    dependsOn ExamplePush, ExamplePull
}
