{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credentials - keep this secret!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  Cluster number, e.g. 10000\n",
    "cluster  = ''\n",
    "\n",
    "# Cluster username\n",
    "username = ''\n",
    "\n",
    "# Cluster password\n",
    "password = ''\n",
    "\n",
    "# file path in HDFS\n",
    "filepath = 'yourpath/yourfile.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your custom code to read_csv_lines for processing your datafile    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "        \n",
    "def read_csv_lines(lines, is_first_chunk = False):\n",
    "    ''' returns a pandas dataframe '''\n",
    "    \n",
    "    if is_first_chunk:\n",
    "        # you will want to set the header here if your datafile has a header record\n",
    "        return pd.read_csv(lines, sep='|', header=None)\n",
    "    else:\n",
    "        return pd.read_csv(lines, sep='|', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "host = 'ehaasp-{0}-mastermanager.bi.services.bluemix.net'.format(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Code to connect to BigInsights on Cloud via WebHDFS - don't change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "if sys.version_info[0] < 3: \n",
    "    from StringIO import StringIO\n",
    "else:\n",
    "    from io import StringIO\n",
    "    \n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "print('SCRIPT START: {0}'.format(datetime.datetime.now()))\n",
    "\n",
    "chunk_size = 10000000 # Read in 100 Mb chunks\n",
    "\n",
    "url = \"https://{0}:8443/gateway/default/webhdfs/v1/{1}?op=OPEN\".format(host, filepath)\n",
    "\n",
    "# note SSL verification is been disabled\n",
    "r = requests.get(url, \n",
    "                 auth=(username, password), \n",
    "                 verify=False, \n",
    "                 allow_redirects=True, \n",
    "                 stream=True)\n",
    "\n",
    "df = None\n",
    "chunk_num = 1\n",
    "remainder = ''\n",
    "for chunk in r.iter_content(chunk_size):\n",
    "    \n",
    "    if chunk: # filter out keep-alive new chunks\n",
    "        \n",
    "        # Show progress by printing a dot - useful when chunk size is quite small\n",
    "        # sys.stdout.write('.')\n",
    "        # sys.stdout.flush()\n",
    "\n",
    "        txt = remainder + chunk\n",
    "        if '\\n' in txt:\n",
    "            [lines, remainder] = txt.rsplit('\\n', 1)\n",
    "        else:\n",
    "            lines = txt\n",
    "\n",
    "        if chunk_num == 1:\n",
    "            pdf = read_csv_lines(StringIO(lines), True)\n",
    "            df = sqlContext.createDataFrame(pdf)\n",
    "        else:\n",
    "            pdf = read_csv_lines(StringIO(lines), False)\n",
    "            df2 = sqlContext.createDataFrame(pdf)\n",
    "            \n",
    "            df = df.sql_ctx.createDataFrame(\n",
    "                    df._sc.union([df.rdd, df2.rdd]), df.schema\n",
    "                    )\n",
    "            \n",
    "        print('Imported chunk: {0} record count: {1} df count: {2}'.format(chunk_num, len(pdf), df.count()))\n",
    "            \n",
    "        chunk_num = chunk_num + 1\n",
    "        \n",
    "print '\\nTotal record import count: {0}'.format(df.count())\n",
    "\n",
    "print('SCRIPT END: {0}'.format(datetime.datetime.now()))\n",
    "\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Add your code here to work with the imported dataframe, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
