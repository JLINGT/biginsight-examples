plugins {
  id 'org.hidetake.ssh' version '1.5.0'
}

Properties props = new Properties()
props.load(new FileInputStream("$projectDir/../../connection.properties"))

// extract BigInsights hostname from the gateway url
def matcher = props.gateway =~ /^(https?:\/\/)([^:^\/]*)(:\d*)?(.*)?.*$/
def hostname = matcher[0][2] 

// setup the connection details for ssh
remotes {
    bicluster {
       host = hostname
       user = props.username
       password = props.password
    }
}

task Example << {

    def tmpDir = "test-${new Date().getTime()}"

    def tmpHdfsDir = "/user/${props.username}/${tmpDir}"

    ssh.run {
        session(remotes.bicluster) {
            // TODO: do we need to run this?
            // kinit -k -t biadmin.keytab biadmin@IBM.COM

            // create localdir for holding files
            execute "mkdir ${tmpDir}"

            // create hdfs for holding files
            execute "hadoop fs -mkdir ${tmpHdfsDir}"

            // upload spark script and text file to process
            put from: "${projectDir}/wordcount.py", into: "${tmpDir}/wordcount.py"
            put from: "${projectDir}/LICENSE", into: "${tmpDir}/LICENSE"

            // put LICENSE into hdfs
            execute "hadoop fs -put ${tmpDir}/LICENSE ${tmpHdfsDir}/LICENSE"

            // execute spark workcount job against the file in hdfs
            execute "pyspark ${tmpDir}/wordcount.py ${tmpHdfsDir}/LICENSE"

            // remove temporary hdfs dir
            execute "hadoop fs -rm -r ${tmpHdfsDir}"

            // remove temporary local dir
            execute "rm -rf ${tmpDir}"
        }
    }
}
