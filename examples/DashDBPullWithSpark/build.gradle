plugins {
  // we use the ssh plugin to execute actions on the server over ssh
  id 'org.hidetake.ssh' version '1.5.0'
}

// load some common helper methods
apply from: "${projectDir}/../../shared/common-helpers.gradle"

Properties props = new Properties()
props.load(new FileInputStream("$projectDir/../../connection.properties"))

task('Example') {

    dependsOn SetupLibs  // SetupLibs task is defined in shared/common-helpers.build

    doLast {
        def tmpDir = "test-${new Date().getTime()}"

        def tmpHdfsDir = "/user/${props.username}/${tmpDir}"
        
        // ssh plugin documentation: https://gradle-ssh-plugin.github.io/docs/
        ssh.run {
            // remotes.bicluster is defined in shared/common-helpers.gradle
            session(remotes.bicluster) {

                try {
                    // initialise kerberos
                    execute "kinit -k -t ${props.username}.keytab ${props.username}@IBM.COM"
                } 
                catch (Exception e) {
                    println "problem running kinit - maybe this is a Basic cluster?"
                }

                // create temp local dir for holding sparkscript
                execute "mkdir ${tmpDir}"

                put from: "${projectDir}/lib/db2jcc.jar",            into: "${tmpDir}/db2jcc.jar"
                put from: "${projectDir}/lib/db2jcc4.jar",           into: "${tmpDir}/db2jcc4.jar"
                put from: "${projectDir}/lib/db2jcc_license_cu.jar", into: "${tmpDir}/db2jcc_license_cu.jar"

                // upload spark script
                put from: "${projectDir}/importfromdashdb.py", into: "${tmpDir}/importfromdashdb.py"

                // create temp hdfs folder for holding exported data
                execute "hadoop fs -mkdir ${tmpHdfsDir}"

                def jarfiles = ["${tmpDir}/db2jcc.jar", "${tmpDir}/db2jcc4.jar", "${tmpDir}/db2jcc_license_cu.jar"]

                def jars = "--jars \"${jarfiles.join(',')}\""
                def dcp  = "--driver-class-path \"${jarfiles.join(':')}\""
                def conf = "--conf \"spark.driver.extraClassPath=${jarfiles.join(':')}\""

                // execute spark job
                execute "pyspark ${conf} ${jars} ${dcp} ${tmpDir}/importfromdashdb.py '${props.dashdb_pull_jdbc_url}' ${tmpHdfsDir}/SALES"

                // print contents of file imported from Cloudant
                execute "hadoop fs -cat ${tmpHdfsDir}/SALES/*"

                // remove temporary hdfs dir
                execute "hadoop fs -rm -r ${tmpHdfsDir}"

                // remove temporary local dir
                execute "rm -rf ${tmpDir}"
            
                println "\nSUCCESS >> Successfully Imported data from dashDB to HDFS"
            }

        }
    }
}
